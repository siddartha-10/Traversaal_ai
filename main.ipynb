{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_experimental.agents import create_csv_agent\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "import os\n",
    "load_dotenv()\n",
    "import pandas as pd\n",
    "from langchain_community.document_loaders import JSONLoader\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = AzureChatOpenAI(openai_api_version=os.environ.get(\"AZURE_OPENAI_VERSION\", \"2023-07-01-preview\"),\n",
    "        azure_deployment=os.environ.get(\"AZURE_OPENAI_DEPLOYMENT\", \"gpt4chat\"),\n",
    "        azure_endpoint=os.environ.get(\"AZURE_OPENAI_ENDPOINT\", \"https://gpt-4-trails.openai.azure.com/\"),\n",
    "        api_key=os.environ.get(\"AZURE_OPENAI_KEY\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'source': '/Users/siddartha/Desktop/github/Traversaal_ai/data.json', 'seq_num': 1, 'location': 'Istanbul', 'price_range': 'low'}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def metadata_func(record: str, metadata: dict) -> dict:\n",
    "    lines = record.split('\\n')\n",
    "    locality_line = lines[10]\n",
    "    price_range_line = lines[12]\n",
    "    locality = locality_line.split(': ')[1]\n",
    "    price_range = price_range_line.split(': ')[1]\n",
    "    metadata[\"location\"] = locality\n",
    "    metadata[\"price_range\"] = price_range\n",
    "\n",
    "    return metadata\n",
    "\n",
    "# Instantiate the JSONLoader with the metadata_func\n",
    "jq_schema = '.parser[] | to_entries | map(\"\\(.key): \\(.value)\") | join(\"\\n\")'\n",
    "loader = JSONLoader(\n",
    "    jq_schema=jq_schema,\n",
    "    file_path='data.json',\n",
    "    metadata_func=metadata_func,\n",
    ")\n",
    "\n",
    "# Load the JSON file and extract metadata\n",
    "documents = loader.load()\n",
    "\n",
    "# Print the metadata of the first document\n",
    "print(documents[0].metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.documents.base.Document'>\n"
     ]
    }
   ],
   "source": [
    "print(type(documents[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = [i.page_content for i in documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import AzureOpenAIEmbeddings\n",
    "embeddings = AzureOpenAIEmbeddings(\n",
    "    openai_api_version=os.environ.get(\"AZURE_OPENAI_VERSION\", \"2023-07-01-preview\"),\n",
    "    azure_deployment=os.environ.get(\"embeddings\",\"embed\"),\n",
    "    azure_endpoint=os.environ.get(\"AZURE_OPENAI_ENDPOINT\", \"https://gpt-4-trails.openai.azure.com/\"),\n",
    "    api_key=os.environ.get(\"AZURE_OPENAI_KEY\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "def get_vectorstore(text_chunks):\n",
    "    embeddings = OpenAIEmbeddings()\n",
    "    vectorstore = FAISS.from_documents(documents=text_chunks, embedding=embeddings)\n",
    "    return vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector = get_vectorstore(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "def api_call(text, previous_messages):\n",
    "  url = \"https://api-ares.traversaal.ai/live/predict\"\n",
    "\n",
    "  payload = { \"query\": [text+previous_messages]}\n",
    "  headers = {\n",
    "    \"x-api-key\": \"ares_a0866ad7d71d2e895c5e05dce656704a9e29ad37860912ad6a45a4e3e6c399b5\",\n",
    "    \"content-type\": \"application/json\"\n",
    "  }\n",
    "\n",
    "  response = requests.post(url, json=payload, headers=headers)\n",
    "\n",
    "  # here we will use the llm to summarize the response received from the ares api\n",
    "  response_data = response.json()\n",
    "  #print(response_data)\n",
    "  try:\n",
    "    response_text = response_data['data']['response_text']\n",
    "    web_urls = response_data['data']['web_url']\n",
    "    # Continue processing the data...\n",
    "  except KeyError:\n",
    "    print(\"Error: Unexpected response from the API. Please try again or contact the api owner.\")\n",
    "    # Optionally, you can log the error or perform other error handling actions.\n",
    "  \n",
    "\n",
    "  if len(response_text) > 10000:\n",
    "    response_text = response_text[:8000]\n",
    "    prompt = f\"Summarize the following text in 500-100 0 words and jsut summarize what you see and do not add anythhing else: {response_text}\"\n",
    "    summary = llm.invoke(prompt)\n",
    "    print(summary)\n",
    "  else:\n",
    "    summary = response_text\n",
    "\n",
    "  result = \"{} My list is: {}\".format(response_text, web_urls)\n",
    "\n",
    "# Convert the result to a string\n",
    "  result_str = str(result)\n",
    "\n",
    "  return result_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "def ares_api_call(query):\n",
    "    url = \"https://api.traversaal.ai/live/predict\"\n",
    "    payload = {\"query\": [query]}\n",
    "    headers = {\n",
    "        \"x-api-key\": \"aares_1790bf3106074d9bb5919475f11292bb774f61ca24ac483bb956b37637dee1cd\",\n",
    "        \"content-type\": \"application/json\"\n",
    "    }\n",
    "    response = requests.post(url, json=payload, headers=headers)\n",
    "\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [405]>"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ares_api_call(\"What is the weather in New York?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.memory import ConversationSummaryMemory\n",
    "\n",
    "template = \"\"\"\n",
    "\n",
    "context:- I have low budget what is the best hotel in Instanbul?\n",
    "anser:- The other hotels in instanbul are costly and are not in your budget. so the best hotel in instanbul for you is hotel is xyz.\"\n",
    "\n",
    "Don’t give information not mentioned in the CONTEXT INFORMATION. \n",
    "The system should take into account various factors such as location, amenities, user reviews, and other relevant criteria to \n",
    "generate informative and personalized explanations.\n",
    "{context} \n",
    "Question: {question}\n",
    "Answer:\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"context\",\"question\"])\n",
    "\n",
    "chain_type_kwargs = {\"prompt\": prompt}\n",
    "chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=vector.as_retriever(),\n",
    "    chain_type_kwargs=chain_type_kwargs,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Grand Hotel du Palais Royal is an expensive hotel in Paris. It is a 5-star hotel located in the heart of the city, near the Palais Royal and its romantic French garden, the Louvre, and the Jardin des Tuileries. The hotel has received mixed reviews. On one hand, a guest mentioned that the lobby was suffocatingly filled with cologne, there was construction going on in the room next to theirs, and there was some confusion about the complimentary breakfast. On the other hand, another guest had a wonderful stay, praising the great staff, location, comfortable rooms, kindness, courtesy, and accuracy. The hotel has an overall rating of 5, based on 1502 reviews. More details can be found at its [website](https://www.tripadvisor.com/Hotel_Review-g187147-d617625-Reviews-or20-Grand_Hotel_du_Palais_Royal-Paris_Ile_de_France.html).\n"
     ]
    }
   ],
   "source": [
    "question = \"give me deatils about expensive hotels in paris?\"\n",
    "\n",
    "final_response = chain.run(question)\n",
    "print(final_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Grand Hotel du Palais Royal, Le Meurice, Plaza Athénée, Shangri-La Paris, Mandarin Oriental, The Ritz Hotel, Four Seasons Hotel, Peninsula Paris, and Rosewood Hotel are some of the most expensive hotels in Paris. These hotels offer luxury accommodations, with prices that can reach up to $36,000 per night. They are located in prestigious areas of the city, near attractions such as the Louvre Museum, the Place de la Concorde, and the Eiffel Tower. The hotels are praised for their luxurious decor, spacious rooms, and excellent service. However, they have also received mixed reviews from guests. More details can be found on their respective websites.\n"
     ]
    }
   ],
   "source": [
    "api_response = api_call(question)\n",
    "prompt = \"\"\"Please write the response to the user query: using the final_response and api_resource and make sure you are\n",
    "The system should take into account various factors such as location, amenities, user reviews, and other relevant criteria to \n",
    "generate informative and personalized explanations. Do not add any information that is not mentioned in the context.\n",
    "and make sure the answer is up to the point and not too long.\n",
    "\n",
    "question: when did sachin hit his 100th century?\n",
    "final_response: I can you assist you with hotel's or travels or food but cannot help other than that..\n",
    "\n",
    "\"\"\"\n",
    "response = llm.invoke(prompt+question+final_response + api_response)\n",
    "\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content=\"Sure, I'd be happy to help. However, I'll need a bit more information. Could you please tell me the names of the hotels and their locations?\"\n"
     ]
    }
   ],
   "source": [
    "print(llm.invoke(\"Can you help me get the hotel links?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import ConversationChain, ConversationalRetrievalChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains.conversational_retrieval.prompts import QA_PROMPT\n",
    "from langchain.prompts.chat import SystemMessagePromptTemplate,HumanMessagePromptTemplate\n",
    "from langchain.prompts import PromptTemplate, ChatPromptTemplate\n",
    "\n",
    "promptTemplate = \"\"\"\n",
    "context:- I have low budget what is the best hotel in Instanbul?\n",
    "anser:- The other hotels in instanbul are costly and are not in your budget. so the best hotel in instanbul for you is hotel is xyz.\"\n",
    "\n",
    "Don’t give information not mentioned in the CONTEXT INFORMATION. \n",
    "The system should take into account various factors such as location, amenities, user reviews, and other relevant criteria to \n",
    "generate informative and personalized explanations.\n",
    "{context} \n",
    "Question: {question}\n",
    "Answer:\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template=promptTemplate, input_variables=[\"context\",\"question\"])\n",
    "\n",
    "chain_type_kwargs = {\"prompt\": prompt}\n",
    "\n",
    "history = []\n",
    "memory = ConversationSummaryMemory(\n",
    "    memory_key='chat_history', return_messages=True, output_key='answer', llm=llm\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    SystemMessagePromptTemplate.from_template(promptTemplate),\n",
    "    HumanMessagePromptTemplate.from_template(\"{question}\")\n",
    "]\n",
    "\n",
    "qa_prompt = ChatPromptTemplate.from_messages(messages)\n",
    "\n",
    "qa_chain = ConversationalRetrievalChain.from_llm(\n",
    "    llm, vector.as_retriever(), memory=memory, get_chat_history=lambda h: h,\n",
    "    combine_docs_chain_kwargs={\"prompt\": qa_prompt}\n",
    ")\n",
    "\n",
    "chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=vector.as_retriever(),\n",
    "    chain_type_kwargs=chain_type_kwargs,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_response(question):\n",
    "    answer_for_message = \"\"\n",
    "    answer = qa_chain.invoke({\"question\":question,\"chat_history\":history})#,return_only_outputs=True)\n",
    "    answer = answer[\"answer\"]\n",
    "    prompt = \"\"\"Please write the response to the user query: using the final_response and api_resource and make sure you are\n",
    "The system should take into account various factors such as location, amenities, user reviews, and other relevant criteria to \n",
    "generate informative and personalized explanations and also provide them with website links. Do not add any information that is not mentioned in the context.\n",
    "and make sure the answer is up to the point and not too long.\n",
    "\n",
    "question: when did sachin hit his 100th century?\n",
    "final_response: I can you assist you with hotel's or travels or food but cannot help other than that..\n",
    "\n",
    "question: what year did messi won the world cup?\n",
    "final_response: I can you assist you with hotel's or travels or food but cannot help other than that.\n",
    "\n",
    "use the above 2 response only when the query is not related to the hotel or travel or food.\n",
    "\n",
    "\"\"\"\n",
    "    if answer_for_message == \"\":\n",
    "        api_answer = api_call(question,answer_for_message)\n",
    "    else:\n",
    "        api_answer = api_call(question,answer_for_message.content)\n",
    "    final_answer = llm.invoke(prompt+question+answer+api_answer)\n",
    "    answer_for_message = llm.invoke(\"Summarize as much as you can but make sure you are notloosing any important information.\"+final_answer.content)\n",
    "    history.append((question, answer_for_message.content))\n",
    "    return final_answer.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the factors such as location, amenities, and user reviews, here are some of the high-priced hotels in Paris:\n",
      "\n",
      "1. [Grand Hotel du Palais Royal](https://www.tripadvisor.com/Hotel_Review-g187147-d617625-Reviews-or20-Grand_Hotel_du_Palais_Royal-Paris_Ile_de_France.html): This 5-star hotel is located in the heart of Paris. It is close to the Palais Royal, the Louvre, and the Jardin des Tuileries. The hotel has a high price range and a rating value of 5 based on 1502 reviews.\n",
      "\n",
      "2. [Le Meurice](https://www.dorchestercollection.com/en/paris/le-meurice/): This luxury hotel offers rooms decorated in the style of King Louis XVI, and its Presidential suite can cost up to $8,800 per night.\n",
      "\n",
      "3. [Plaza Athénée](https://www.dorchestercollection.com/en/paris/hotel-plaza-athenee/): This hotel has spacious rooms with marble bathrooms and beautiful dressing rooms. The Eiffel Suite can cost up to $15,000 per night.\n",
      "\n",
      "4. [Shangri-La Paris](https://www.shangri-la.com/paris/shangrila/): This hotel offers rooms with a view of the Eiffel Tower. The most expensive suite, La Suite Shangri-La, can cost up to $21,000 per night.\n",
      "\n",
      "5. [Mandarin Oriental](https://www.mandarinoriental.com/paris/place-vendome/luxury-hotel): This hotel offers a Royale Mandarin Suite, which can cost up to $22,240 per night. \n",
      "\n",
      "6. [The Ritz Hotel](https://www.ritzparis.com/en-GB): Known as a royal palace, this hotel offers suites with antique furniture and gold details.\n",
      "\n",
      "7. [Four Seasons Hotel](https://www.fourseasons.com/paris/): This hotel features classic décor, spacious rooms, and luxurious balconies. The most expensive suites can go up to $28,000 per night.\n",
      "\n",
      "8. [Peninsula Paris](https://www.peninsula.com/en/paris/5-star-luxury-hotel-16th-arrondissement): The most expensive suite, The Peninsula Suite, can cost up to $29,900 per night.\n",
      "\n",
      "9. [Rosewood Hotel](https://www.rosewoodhotels.com/en/hotel-de-crillon): The most expensive hotel in Paris, it offers two luxury suites called Les Grands Appartements for up to $36,000 per night.\n",
      "\n",
      "These hotels are known for their luxurious amenities and high price range. Please note that the prices may vary.\n"
     ]
    }
   ],
   "source": [
    "print(chat_response(\"give me deatils about expensive hotels in paris?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure, here are the website links for the hotels mentioned above:\n",
      "\n",
      "1. Radisson Blu Hotel Istanbul Ottomare: [https://www.tripadvisor.com/Hotel_Review-g293974-d9459106-Reviews-or30-Radisson_Blu_Hotel_Istanbul_Ottomare-Istanbul.html](https://www.tripadvisor.com/Hotel_Review-g293974-d9459106-Reviews-or30-Radisson_Blu_Hotel_Istanbul_Ottomare-Istanbul.html)\n",
      "\n",
      "2. Park Grand London Hyde Park: [https://www.tripadvisor.com/Hotel_Review-g186338-d3164384-Reviews-or10-Park_Grand_London_Hyde_Park-London_England.html](https://www.tripadvisor.com/Hotel_Review-g186338-d3164384-Reviews-or10-Park_Grand_London_Hyde_Park-London_England.html)\n",
      "\n",
      "3. Wyndham Grand Istanbul Kalamis Marina Hotel: [https://www.tripadvisor.com/Hotel_Review-g293974-d3588557-Reviews-or30-Wyndham_Grand_Istanbul_Kalamis_Marina_Hotel-Istanbul.html](https://www.tripadvisor.com/Hotel_Review-g293974-d3588557-Reviews-or30-Wyndham_Grand_Istanbul_Kalamis_Marina_Hotel-Istanbul.html)\n",
      "\n",
      "4. Royal National Hotel: [https://www.tripadvisor.com/Hotel_Review-g186338-d218407-Reviews-or10-Royal_National_Hotel-London_England.html](https://www.tripadvisor.com/Hotel_Review-g186338-d218407-Reviews-or10-Royal_National_Hotel-London_England.html)\n",
      "\n",
      "Please note that these are links to reviews of the hotels on TripAdvisor. You can find more information about them and potentially book your stay through these links.\n"
     ]
    }
   ],
   "source": [
    "print(chat_response(\"Can you give me the links of websites for the above hotels?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'llm_chain'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[205], line 89\u001b[0m\n\u001b[1;32m     86\u001b[0m tools \u001b[38;5;241m=\u001b[39m [chain_rag_tool, api_tool]\n\u001b[1;32m     87\u001b[0m \u001b[38;5;66;03m# llm_with_tools = llm.bind_tools(tools)\u001b[39;00m\n\u001b[0;32m---> 89\u001b[0m custom_agent \u001b[38;5;241m=\u001b[39m \u001b[43mCustomAgent\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtools\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mllm_chain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mllm\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     90\u001b[0m executor \u001b[38;5;241m=\u001b[39m AgentExecutor(agent\u001b[38;5;241m=\u001b[39mcustom_agent)\n\u001b[1;32m     92\u001b[0m \u001b[38;5;66;03m# Define a function to get response for user input\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[205], line 56\u001b[0m, in \u001b[0;36mCustomAgent.__init__\u001b[0;34m(self, tools, llm_chain, **kwargs)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, tools, llm_chain, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m---> 56\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtools \u001b[38;5;241m=\u001b[39m tools\n\u001b[1;32m     58\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm_chain \u001b[38;5;241m=\u001b[39m llm_chain\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:180\u001b[0m, in \u001b[0;36mdeprecated.<locals>.deprecate.<locals>.finalize.<locals>.warn_if_direct_instance\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    178\u001b[0m     warned \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    179\u001b[0m     emit_warning()\n\u001b[0;32m--> 180\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pydantic/v1/main.py:339\u001b[0m, in \u001b[0;36mBaseModel.__init__\u001b[0;34m(__pydantic_self__, **data)\u001b[0m\n\u001b[1;32m    333\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    334\u001b[0m \u001b[38;5;124;03mCreate a new model by parsing and validating input data from keyword arguments.\u001b[39;00m\n\u001b[1;32m    335\u001b[0m \n\u001b[1;32m    336\u001b[0m \u001b[38;5;124;03mRaises ValidationError if the input data cannot be parsed to form a valid model.\u001b[39;00m\n\u001b[1;32m    337\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    338\u001b[0m \u001b[38;5;66;03m# Uses something other than `self` the first arg to allow \"self\" as a settable attribute\u001b[39;00m\n\u001b[0;32m--> 339\u001b[0m values, fields_set, validation_error \u001b[38;5;241m=\u001b[39m \u001b[43mvalidate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m__pydantic_self__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__class__\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    340\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m validation_error:\n\u001b[1;32m    341\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m validation_error\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pydantic/v1/main.py:1102\u001b[0m, in \u001b[0;36mvalidate_model\u001b[0;34m(model, input_data, cls)\u001b[0m\n\u001b[1;32m   1100\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m   1101\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1102\u001b[0m     values \u001b[38;5;241m=\u001b[39m \u001b[43mvalidator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcls_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1103\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mValueError\u001b[39;00m, \u001b[38;5;167;01mTypeError\u001b[39;00m, \u001b[38;5;167;01mAssertionError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m   1104\u001b[0m     errors\u001b[38;5;241m.\u001b[39mappend(ErrorWrapper(exc, loc\u001b[38;5;241m=\u001b[39mROOT_KEY))\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/langchain/agents/agent.py:697\u001b[0m, in \u001b[0;36mAgent.validate_prompt\u001b[0;34m(cls, values)\u001b[0m\n\u001b[1;32m    694\u001b[0m \u001b[38;5;129m@root_validator\u001b[39m()\n\u001b[1;32m    695\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mvalidate_prompt\u001b[39m(\u001b[38;5;28mcls\u001b[39m, values: Dict) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dict:\n\u001b[1;32m    696\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Validate that prompt matches format.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 697\u001b[0m     prompt \u001b[38;5;241m=\u001b[39m \u001b[43mvalues\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mllm_chain\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mprompt\n\u001b[1;32m    698\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124magent_scratchpad\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m prompt\u001b[38;5;241m.\u001b[39minput_variables:\n\u001b[1;32m    699\u001b[0m         logger\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[1;32m    700\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`agent_scratchpad` should be a variable in prompt.input_variables.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    701\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Did not find it, so adding it at the end.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    702\u001b[0m         )\n",
      "\u001b[0;31mKeyError\u001b[0m: 'llm_chain'"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Qdrant\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.agents import Tool, Agent, AgentType\n",
    "from langchain.agents import AgentExecutor\n",
    "from langchain_core.tools import Tool\n",
    "\n",
    "# Define the API call function for Ares API\n",
    "def ares_api_call(query):\n",
    "    url = \"https://api.traversaal.ai/live/predict\"\n",
    "    payload = { \"query\": query }\n",
    "    headers = {\n",
    "        \"x-api-key\": \"ares_a0866ad7d71d2e895c5e05dce656704a9e29ad37860912ad6a45a4e3e6c399b5\",\n",
    "        \"content-type\": \"application/json\"\n",
    "    }\n",
    "    response = requests.post(url, json=payload, headers=headers)\n",
    "    # Process API response and return relevant details\n",
    "    return response.json()\n",
    "\n",
    "def search():\n",
    "    llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
    "    vector=vector\n",
    "    return RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=vector.as_retriever(),\n",
    "    chain_type_kwargs=chain_type_kwargs,\n",
    ")\n",
    "\n",
    "# Initialize LangChain tools\n",
    "template = \"\"\"\n",
    "\n",
    "context:- I have low budget what is the best hotel in Instanbul?\n",
    "anser:- The other hotels in instanbul are costly and are not in your budget. so the best hotel in instanbul for you is hotel is xyz.\"\n",
    "\n",
    "Don’t give information not mentioned in the CONTEXT INFORMATION. \n",
    "The system should take into account various factors such as location, amenities, user reviews, and other relevant criteria to \n",
    "generate informative and personalized explanations.\n",
    "{context} \n",
    "Question: {question}\n",
    "Answer:\"\"\"\n",
    "\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "\n",
    "\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"context\",\"question\"])\n",
    "\n",
    "chain_type_kwargs = {\"prompt\": prompt}\n",
    "\n",
    "\n",
    "api_tool = Tool(name=\"Ares API\",\n",
    "                func=ares_api_call,\n",
    "                description=\"Integration with Traversaal AI Ares API for real-time internet searches.\"\n",
    "               )\n",
    "\n",
    "chain_rag_tool = Tool(name=\"RAG Chain\",\n",
    "                      func=search.run,\n",
    "                      description=\"RAG chain for question answering.\"\n",
    "                     )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "tools = [chain_rag_tool, api_tool]\n",
    "lm_with_tools = llm.bind(functions=[format_tool_to_openai_function(t) for t in tools])\n",
    "\n",
    "\n",
    "executor = AgentExecutor()\n",
    "\n",
    "# Define a function to get response for user input\n",
    "def get_response(user_input):\n",
    "    response = executor.run(user_input)\n",
    "    return response\n",
    "\n",
    "# Define a function to start the conversation with the user\n",
    "def start_conversation():\n",
    "    print(\"Chatbot: Hi there! How can I assist you with your hotel preferences today?\")\n",
    "    while True:\n",
    "        user_input = input(\"User: \")\n",
    "        if user_input.lower() in ['exit', 'quit', 'bye']:\n",
    "            print(\"Chatbot: Goodbye! Have a great day.\")\n",
    "            break\n",
    "        response = get_response(user_input)\n",
    "        print(\"Chatbot:\", response)\n",
    "\n",
    "# Start the conversation\n",
    "start_conversation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
